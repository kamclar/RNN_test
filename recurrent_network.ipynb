{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('INFO')\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, SimpleRNN, TimeDistributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output:  [[0.26372457 2.5810895 ]]\n",
      "state:  [[0.26372457 2.5810895 ]]\n",
      "~~~~~~~~~~~~~~\n",
      "weights:  [array([[-1.0481719 , -0.01463342],\n",
      "       [ 1.1329073 ,  0.528082  ]], dtype=float32), array([[ 0.9881021 ,  0.1537993 ],\n",
      "       [-0.1537993 ,  0.98810214]], dtype=float32), array([0., 0.], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# data\n",
    "# two input vectors = two timesteps\n",
    "x = np.array([[[2,2], [3,3]]])\n",
    "\n",
    "# input/output shape\n",
    "inputs = Input((2,2)) # just one timestep\n",
    "outputs = 2\n",
    "\n",
    "# MODEL\n",
    "output, state = SimpleRNN(outputs, return_state=True, activation='relu')(inputs)\n",
    "model = Model(inputs=inputs, outputs=[output, state])\n",
    "\n",
    "# print output and state\n",
    "output, state = model.predict(x)\n",
    "print('output: ', output)\n",
    "print('state: ', state)\n",
    "print('~~~~~~~~~~~~~~')\n",
    "print('weights: ', model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'ReadVariableOp:0' shape=(2, 2) dtype=float32>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weights[1].value()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### weights[0] is the input matrix [input_dim, output_dim]\n",
    "### weights[1] is the recurent matrix [output_dim, output_dim]\n",
    "### weights[2] is the bias matrix [output_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 2, 2)]            0         \n",
      "_________________________________________________________________\n",
      "simple_rnn (SimpleRNN)       [(None, 2), (None, 2)]    10        \n",
      "=================================================================\n",
      "Total params: 10\n",
      "Trainable params: 10\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy implementace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    ex = np.exp(x - np.max(x))\n",
    "    return ex / ex.sum(axis=0)\n",
    "def relu(x):\n",
    "    return np.maximum(x, 0)\n",
    "def sigmoid(x):\n",
    "    return 1 / (1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  epoch loss:  5.094590652078127\n",
      "4000  epoch loss:  0.1186453592454508\n",
      "8000  epoch loss:  0.03458781187256001\n",
      "12000  epoch loss:  0.012676807364435038\n",
      "16000  epoch loss:  0.005093823554751602\n",
      "y hat:  [0. 1. 2. 3.]\n",
      "true :  [[0 1 2 3]]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[0,0,0,1],[0,0,1,1],[0,1,1,1], [1,1,1,1]])\n",
    "y = np.array([[0],[1],[2],[3]])\n",
    "\n",
    "lr = 0.001\n",
    "\n",
    "# lets try random weight that is just one shared number\n",
    "Wx = np.random.randn()\n",
    "Wa = np.random.randn()\n",
    "\n",
    "state = np.zeros((X.shape[0],X.shape[1] + 1)) # has to be one column wider because of the State zero\n",
    "gradient = np.zeros((X.shape))\n",
    "states_in = np.zeros((X.shape[0],X.shape[1]))\n",
    "states_out = np.zeros((X.shape[0],X.shape[1]))\n",
    "\n",
    "# A training loop\n",
    "for i in range(epochs): \n",
    "    # A propagation of input and recurrency through the cells \n",
    "    for j in range(X.shape[1]-1):\n",
    "        states_in[j] = state[:,j]*Wa + X[:,j]*Wx\n",
    "        states_out[j] = sigmoid(states_in[j])\n",
    "        state[:,j+1] = states_out[j]\n",
    "    \n",
    "    # output from the net\n",
    "    states_in[j+1]  = state[:,j+1]*Wa + X[:,j+1]*Wx\n",
    "    state[:,j+2] = states_in[j+1]     \n",
    "    \n",
    "    # cost function is MSE\n",
    "    loss = np.square(state[:,j+2] - np.squeeze(y)).sum() / len(x)\n",
    "    \n",
    "    # let's print output every n-th epoch\n",
    "    if i % 4000 == 0:\n",
    "        print( i, \" epoch\", \"loss: \", loss) \n",
    "     \n",
    "    # time for backprop\n",
    "    grad_Wx = 0\n",
    "    grad_Wa = 0\n",
    "    gradient[:,vec_end-1] = (state[:,vec_end] - np.squeeze(y)) * (2/X.shape[1])\n",
    "\n",
    "    # propagate the error through every cell in reverse order\n",
    "    for i in range(X.shape[1]-1, 0, -1):        \n",
    "        grad_Wx += gradient[:,i]*X[:,i-1]\n",
    "        grad_Wa += gradient[:,i]*state[:,i-1]\n",
    "        gradient[:,i-1] = gradient[:,i] * Wa  * relu(states_in[i])\n",
    "\n",
    "    # update the shared weight\n",
    "    Wx = Wx - lr * grad_Wx\n",
    "    Wa = Wa - lr * grad_Wa\n",
    "\n",
    "# make a prediction\n",
    "for j in range(x.shape[1]-1):\n",
    "    states_in[j] = state[:,j]*Wa + X[:,j]*Wx\n",
    "    states_out[j] = sigmoid(states_in[j])\n",
    "    state[:,j+1] = states_out[j]\n",
    "\n",
    "    \n",
    "states_in[j+1]  = state[:,j+1]*Wa + X[:,j+1]*Wx\n",
    "state[:,j+2] = states_in[j+1]\n",
    "     \n",
    "print(\"y hat: \",np.round(states_in[j+1]))\n",
    "print(\"true : \",y.T) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
